{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac45ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6140c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer        # ignore warnings\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "865b8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset\n",
    "\n",
    "data = pd.read_csv(\"news.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ded2e072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6903</td>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7341</td>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>95</td>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4869</td>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2909</td>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "5        6903                                        Tehran, USA   \n",
       "6        7341  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7          95                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8        4869  Fact check: Trump and Clinton at the 'commande...   \n",
       "9        2909  Iran reportedly makes new push for uranium con...   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  \n",
       "5    \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6  Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7  A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8  Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9  Iranian negotiators reportedly have made a las...  REAL  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d92acad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tehran, USA</td>\n",
       "      <td>\\nI’m not an immigrant, but my grandparents ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Girl Horrified At What She Watches Boyfriend D...</td>\n",
       "      <td>Share This Baylee Luciani (left), Screenshot o...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‘Britain’s Schindler’ Dies at 106</td>\n",
       "      <td>A Czech stockbroker who saved more than 650 Je...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fact check: Trump and Clinton at the 'commande...</td>\n",
       "      <td>Hillary Clinton and Donald Trump made some ina...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iran reportedly makes new push for uranium con...</td>\n",
       "      <td>Iranian negotiators reportedly have made a las...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "5                                        Tehran, USA   \n",
       "6  Girl Horrified At What She Watches Boyfriend D...   \n",
       "7                  ‘Britain’s Schindler’ Dies at 106   \n",
       "8  Fact check: Trump and Clinton at the 'commande...   \n",
       "9  Iran reportedly makes new push for uranium con...   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  \n",
       "5    \\nI’m not an immigrant, but my grandparents ...  FAKE  \n",
       "6  Share This Baylee Luciani (left), Screenshot o...  FAKE  \n",
       "7  A Czech stockbroker who saved more than 650 Je...  REAL  \n",
       "8  Hillary Clinton and Donald Trump made some ina...  REAL  \n",
       "9  Iranian negotiators reportedly have made a las...  REAL  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing \n",
    "\n",
    "''' \n",
    "dataset contain 1 un-named col remove that\n",
    "'''\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dddc9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Encoding\n",
    "\n",
    "''' \n",
    "It converts the categorical column (label in our case) into numerical values.\n",
    "\n",
    "le.fit(data['label']): Fits the encoder on the 'label' column to learn the unique categories.\n",
    "data['label'] = le.transform(data['label']):\n",
    "Transforms the categorical labels into numerical format (0 for REAL, 1 for FAKE).\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['label'])\n",
    "data['label'] = le.transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca85520a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n| **Variable**            | **Purpose**                                                                   |\\n| ----------------------- | ----------------------------------------------------------------------------- |\\n| `embedding_dim = 50`    | Sets the size of word vectors (from GloVe). Each word becomes a 50D vector.   |\\n| `max_length = 54`       | Maximum number of tokens per text (e.g., title). Used for padding/truncating. |\\n| `padding_type = \\'post\\'` | Adds zeros at the end of sequences shorter than `max_length`.                 |\\n| `trunc_type = \\'post\\'`   | Cuts off extra tokens at the end of long sequences.                           |\\n| `oov_tok = \"<OOV>\"`     | Special token for words not seen during training (Out-Of-Vocabulary).         |\\n| `training_size = 3000`  | Uses only the first 3000 samples for training/testing (for speed/memory).     |\\n| `test_portion = 0.1`    | 10% of `training_size` is used as a validation/test set (300 samples).        |\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables Setup\n",
    "\n",
    "''' \n",
    "These are some variables needed to be setup for the model training.\n",
    "'''\n",
    "\n",
    "embedding_dim = 50\n",
    "max_length = 54\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 3000\n",
    "test_portion = 0.1\n",
    "\n",
    "\n",
    "''' \n",
    "\n",
    "| **Variable**            | **Purpose**                                                                   |\n",
    "| ----------------------- | ----------------------------------------------------------------------------- |\n",
    "| `embedding_dim = 50`    | Sets the size of word vectors (from GloVe). Each word becomes a 50D vector.   |\n",
    "| `max_length = 54`       | Maximum number of tokens per text (e.g., title). Used for padding/truncating. |\n",
    "| `padding_type = 'post'` | Adds zeros at the end of sequences shorter than `max_length`.                 |\n",
    "| `trunc_type = 'post'`   | Cuts off extra tokens at the end of long sequences.                           |\n",
    "| `oov_tok = \"<OOV>\"`     | Special token for words not seen during training (Out-Of-Vocabulary).         |\n",
    "| `training_size = 3000`  | Uses only the first 3000 samples for training/testing (for speed/memory).     |\n",
    "| `test_portion = 0.1`    | 10% of `training_size` is used as a validation/test set (300 samples).        |\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2fd5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "''' \n",
    "This process divides a large piece of continuous text into distinct units or tokens. \n",
    "Here we use columns separately for a temporal basis as a pipeline just for good accuracy.\n",
    "\n",
    "tokenizer1.fit_on_texts(title): Fits the tokenizer on the 'title' column to create a vocabulary.\n",
    "pad_sequences(sequences1): Pads the sequences to ensure they all have the same length.\n",
    "\n",
    "'''\n",
    "\n",
    "title = []\n",
    "text = []\n",
    "labels = []\n",
    "\n",
    "for x in range(training_size):\n",
    "    title.append(data['title'][x])\n",
    "    text.append(data['text'][x])\n",
    "    labels.append(data['label'][x])\n",
    "\n",
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(title)\n",
    "word_index1 = tokenizer1.word_index\n",
    "vocab_size1 = len(word_index1)\n",
    "sequences1 = tokenizer1.texts_to_sequences(title)\n",
    "padded1 = pad_sequences(sequences1, padding=padding_type, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f96cd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n| Line of Code                                | Purpose                                 | Calculation / Explanation                    |\\n| ------------------------------------------- | --------------------------------------- | -------------------------------------------- |\\n| `split = int(test_portion * training_size)` | Calculates how many samples for testing | `split = int(0.1 * 3000) = 300`              |\\n| `training_sequences1 = padded1[split:]`     | Gets training input sequences           | `padded1[300:3000]` → 2700 samples           |\\n| `test_sequences1 = padded1[:split]`         | Gets test input sequences               | `padded1[0:300]` → first 300 samples         |\\n| `test_labels = labels[:split]`              | Gets labels for the test set            | `labels[0:300]` → first 300 labels           |\\n| `training_labels = labels[split:]`          | Gets labels for the training set        | `labels[300:3000]` → labels for 2700 samples |\\n\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Splitting Data for Training and Testing\n",
    "\n",
    "''' \n",
    "\n",
    "training_sequences1, test_sequences1: Splits the tokenized and padded data into training and testing sets.\n",
    "training_labels, test_labels: Splits the corresponding labels into training and testing labels.\n",
    "\n",
    "'''\n",
    "\n",
    "split = int(test_portion * training_size)\n",
    "training_sequences1 = padded1[split: training_size]\n",
    "test_sequences1 = padded1[0:split]\n",
    "test_labels = labels[0:split]\n",
    "training_labels = labels[split:training_size]\n",
    "\n",
    "''' \n",
    "\n",
    "| Line of Code                                | Purpose                                 | Calculation / Explanation                    |\n",
    "| ------------------------------------------- | --------------------------------------- | -------------------------------------------- |\n",
    "| `split = int(test_portion * training_size)` | Calculates how many samples for testing | `split = int(0.1 * 3000) = 300`              |\n",
    "| `training_sequences1 = padded1[split:]`     | Gets training input sequences           | `padded1[300:3000]` → 2700 samples           |\n",
    "| `test_sequences1 = padded1[:split]`         | Gets test input sequences               | `padded1[0:300]` → first 300 samples         |\n",
    "| `test_labels = labels[:split]`              | Gets labels for the test set            | `labels[0:300]` → first 300 labels           |\n",
    "| `training_labels = labels[split:]`          | Gets labels for the training set        | `labels[300:3000]` → labels for 2700 samples |\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf79f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Data for LSTM\n",
    "\n",
    "''' \n",
    "We will be using LSTM(Long Short Term Memory) model for prediction and for that we \n",
    "need to reshape padded sequence. We are converting it into np.array() as we need training\n",
    "and test sequences into NumPy arrays which are required by TensorFlow models.\n",
    "'''\n",
    "\n",
    "training_sequences1 = np.array(training_sequences1)\n",
    "test_sequences1 = np.array(test_sequences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2daef999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Word Embedding\n",
    "\n",
    "''' \n",
    "Embeddings allows words with similar meanings to have a similar representation.\n",
    "Here each individual word is represented as real-valued vectors in a predefined \n",
    "vector space. For that we will be using glove.6B.50d.txt.\n",
    "\n",
    "!wget: Downloads the pre-trained GloVe embeddings from the following link.\n",
    "!unzip: Unzips the downloaded file containing the GloVe embeddings\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# Constants\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_url = \"https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\"\n",
    "glove_file = \"glove.6B.50d.txt\"  # Make sure this matches your embedding_dim\n",
    "embedding_dim = 50  # Set according to the GloVe file (50d, 100d, etc.)\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(glove_zip):\n",
    "    print(\"Downloading GloVe vectors....\")\n",
    "    urllib.request.urlretrieve(glove_url, glove_zip)\n",
    "    print(\"Downloaded.\")\n",
    "\n",
    "if not os.path.exists(glove_file):\n",
    "    print(\"Extracting GloVe vectors...\")\n",
    "    with zipfile.ZipFile(glove_zip,'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(\"Extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a5f24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix created sucessfully...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Load embeddings into a dictionary\n",
    "\n",
    "embedding_index = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Create the embedding matrix\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size1 + 1 , embedding_dim))\n",
    "for word , i in word_index1.items():\n",
    "    if i < vocab_size1:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"Embedding matrix created sucessfully...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd78157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 54, 50)            377600    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 54, 50)            0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 50, 64)            16064     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 12, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 426,753\n",
      "Trainable params: 49,153\n",
      "Non-trainable params: 377,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Model Architecture\n",
    "\n",
    "'''  \n",
    "\n",
    "Here we use the TensorFlow embedding technique with Keras Embedding Layer where we map \n",
    "original input data into some set of real-valued dimensions.\n",
    "\n",
    "Embedding: The embedding layer uses pre-trained GloVe embeddings.\n",
    "Conv1D: A 1D convolutional layer to detect patterns in the text.\n",
    "LSTM(64): An LSTM layer to capture long-term dependencies in the data.\n",
    "\n",
    "'''\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size1 + 1, embedding_dim, input_length=max_length,\n",
    "                              weights=[embedding_matrix], trainable=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(64,5,activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c441c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "85/85 - 9s - loss: 0.6460 - accuracy: 0.6126 - val_loss: 0.5955 - val_accuracy: 0.6633 - 9s/epoch - 108ms/step\n",
      "Epoch 2/50\n",
      "85/85 - 1s - loss: 0.5948 - accuracy: 0.6774 - val_loss: 0.5493 - val_accuracy: 0.7067 - 1s/epoch - 13ms/step\n",
      "Epoch 3/50\n",
      "85/85 - 1s - loss: 0.5542 - accuracy: 0.7189 - val_loss: 0.5307 - val_accuracy: 0.6867 - 1s/epoch - 13ms/step\n",
      "Epoch 4/50\n",
      "85/85 - 1s - loss: 0.5127 - accuracy: 0.7474 - val_loss: 0.4914 - val_accuracy: 0.7200 - 1s/epoch - 13ms/step\n",
      "Epoch 5/50\n",
      "85/85 - 1s - loss: 0.4754 - accuracy: 0.7759 - val_loss: 0.4818 - val_accuracy: 0.7367 - 1s/epoch - 12ms/step\n",
      "Epoch 6/50\n",
      "85/85 - 1s - loss: 0.4141 - accuracy: 0.8167 - val_loss: 0.4915 - val_accuracy: 0.7533 - 1s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "85/85 - 1s - loss: 0.3859 - accuracy: 0.8293 - val_loss: 0.4781 - val_accuracy: 0.7700 - 1s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "85/85 - 1s - loss: 0.3419 - accuracy: 0.8496 - val_loss: 0.4851 - val_accuracy: 0.7700 - 1s/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "85/85 - 1s - loss: 0.3019 - accuracy: 0.8726 - val_loss: 0.5547 - val_accuracy: 0.7633 - 1s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "85/85 - 1s - loss: 0.2696 - accuracy: 0.8844 - val_loss: 0.5544 - val_accuracy: 0.7533 - 1s/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "85/85 - 1s - loss: 0.2435 - accuracy: 0.9011 - val_loss: 0.5661 - val_accuracy: 0.7567 - 1s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "85/85 - 1s - loss: 0.2071 - accuracy: 0.9107 - val_loss: 0.6548 - val_accuracy: 0.7567 - 1s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "85/85 - 1s - loss: 0.1971 - accuracy: 0.9200 - val_loss: 0.7358 - val_accuracy: 0.7667 - 1s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "85/85 - 1s - loss: 0.1719 - accuracy: 0.9374 - val_loss: 0.5940 - val_accuracy: 0.7567 - 1s/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "85/85 - 1s - loss: 0.1743 - accuracy: 0.9307 - val_loss: 0.6316 - val_accuracy: 0.7567 - 1s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "85/85 - 1s - loss: 0.1698 - accuracy: 0.9296 - val_loss: 0.6966 - val_accuracy: 0.7733 - 1s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "85/85 - 1s - loss: 0.1452 - accuracy: 0.9385 - val_loss: 0.6466 - val_accuracy: 0.7433 - 1s/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "85/85 - 1s - loss: 0.1406 - accuracy: 0.9463 - val_loss: 0.6478 - val_accuracy: 0.7800 - 1s/epoch - 16ms/step\n",
      "Epoch 19/50\n",
      "85/85 - 1s - loss: 0.1411 - accuracy: 0.9437 - val_loss: 0.8397 - val_accuracy: 0.7400 - 1s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "85/85 - 1s - loss: 0.1422 - accuracy: 0.9481 - val_loss: 0.6711 - val_accuracy: 0.7233 - 1s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "85/85 - 1s - loss: 0.1282 - accuracy: 0.9515 - val_loss: 0.6590 - val_accuracy: 0.7533 - 1s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "85/85 - 1s - loss: 0.1256 - accuracy: 0.9515 - val_loss: 0.5626 - val_accuracy: 0.7600 - 1s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "85/85 - 1s - loss: 0.1104 - accuracy: 0.9596 - val_loss: 0.7922 - val_accuracy: 0.7433 - 1s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "85/85 - 1s - loss: 0.1201 - accuracy: 0.9559 - val_loss: 0.6280 - val_accuracy: 0.7867 - 1s/epoch - 14ms/step\n",
      "Epoch 25/50\n",
      "85/85 - 1s - loss: 0.1103 - accuracy: 0.9570 - val_loss: 0.6794 - val_accuracy: 0.7867 - 1s/epoch - 14ms/step\n",
      "Epoch 26/50\n",
      "85/85 - 1s - loss: 0.0866 - accuracy: 0.9678 - val_loss: 0.7226 - val_accuracy: 0.7600 - 1s/epoch - 14ms/step\n",
      "Epoch 27/50\n",
      "85/85 - 1s - loss: 0.0971 - accuracy: 0.9622 - val_loss: 0.7139 - val_accuracy: 0.7967 - 1s/epoch - 13ms/step\n",
      "Epoch 28/50\n",
      "85/85 - 1s - loss: 0.0961 - accuracy: 0.9626 - val_loss: 0.6868 - val_accuracy: 0.7700 - 1s/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "85/85 - 1s - loss: 0.0832 - accuracy: 0.9659 - val_loss: 0.8114 - val_accuracy: 0.7633 - 1s/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "85/85 - 1s - loss: 0.1017 - accuracy: 0.9663 - val_loss: 0.8593 - val_accuracy: 0.7500 - 1s/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "85/85 - 1s - loss: 0.0908 - accuracy: 0.9622 - val_loss: 0.8066 - val_accuracy: 0.7700 - 1s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "85/85 - 1s - loss: 0.0602 - accuracy: 0.9767 - val_loss: 0.8493 - val_accuracy: 0.7733 - 1s/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "85/85 - 1s - loss: 0.0812 - accuracy: 0.9670 - val_loss: 0.7065 - val_accuracy: 0.7300 - 1s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "85/85 - 1s - loss: 0.0667 - accuracy: 0.9763 - val_loss: 0.9645 - val_accuracy: 0.7533 - 1s/epoch - 14ms/step\n",
      "Epoch 35/50\n",
      "85/85 - 1s - loss: 0.0742 - accuracy: 0.9719 - val_loss: 0.8856 - val_accuracy: 0.7433 - 1s/epoch - 13ms/step\n",
      "Epoch 36/50\n",
      "85/85 - 1s - loss: 0.0667 - accuracy: 0.9759 - val_loss: 0.8043 - val_accuracy: 0.7400 - 1s/epoch - 16ms/step\n",
      "Epoch 37/50\n",
      "85/85 - 1s - loss: 0.0661 - accuracy: 0.9759 - val_loss: 0.8246 - val_accuracy: 0.7567 - 1s/epoch - 15ms/step\n",
      "Epoch 38/50\n",
      "85/85 - 1s - loss: 0.0825 - accuracy: 0.9685 - val_loss: 0.7625 - val_accuracy: 0.7633 - 1s/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "85/85 - 1s - loss: 0.0644 - accuracy: 0.9778 - val_loss: 0.8790 - val_accuracy: 0.7533 - 1s/epoch - 12ms/step\n",
      "Epoch 40/50\n",
      "85/85 - 1s - loss: 0.0742 - accuracy: 0.9741 - val_loss: 0.8783 - val_accuracy: 0.7300 - 1s/epoch - 13ms/step\n",
      "Epoch 41/50\n",
      "85/85 - 1s - loss: 0.0509 - accuracy: 0.9833 - val_loss: 0.8708 - val_accuracy: 0.7767 - 1s/epoch - 12ms/step\n",
      "Epoch 42/50\n",
      "85/85 - 1s - loss: 0.0711 - accuracy: 0.9741 - val_loss: 0.8113 - val_accuracy: 0.7500 - 1s/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "85/85 - 1s - loss: 0.0625 - accuracy: 0.9756 - val_loss: 0.9485 - val_accuracy: 0.7400 - 1s/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "85/85 - 1s - loss: 0.0609 - accuracy: 0.9804 - val_loss: 0.9451 - val_accuracy: 0.7533 - 1s/epoch - 14ms/step\n",
      "Epoch 45/50\n",
      "85/85 - 1s - loss: 0.0678 - accuracy: 0.9733 - val_loss: 0.9564 - val_accuracy: 0.7333 - 1s/epoch - 15ms/step\n",
      "Epoch 46/50\n",
      "85/85 - 1s - loss: 0.0624 - accuracy: 0.9752 - val_loss: 1.0198 - val_accuracy: 0.7400 - 1s/epoch - 13ms/step\n",
      "Epoch 47/50\n",
      "85/85 - 1s - loss: 0.0778 - accuracy: 0.9719 - val_loss: 0.8714 - val_accuracy: 0.7633 - 1s/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "85/85 - 1s - loss: 0.0633 - accuracy: 0.9767 - val_loss: 0.9188 - val_accuracy: 0.7233 - 1s/epoch - 14ms/step\n",
      "Epoch 49/50\n",
      "85/85 - 1s - loss: 0.0630 - accuracy: 0.9778 - val_loss: 0.9291 - val_accuracy: 0.7500 - 1s/epoch - 13ms/step\n",
      "Epoch 50/50\n",
      "85/85 - 1s - loss: 0.0549 - accuracy: 0.9833 - val_loss: 1.0686 - val_accuracy: 0.7333 - 1s/epoch - 12ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' \n",
    "model architecture is ready we can use this to train our model\n",
    "'''\n",
    "\n",
    "#  Training the Model\n",
    "\n",
    "history = model.fit(\n",
    "    training_sequences1,\n",
    "    np.array(training_labels),\n",
    "    epochs=50,\n",
    "    validation_data=(test_sequences1, np.array(test_labels)),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f105c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This news is True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample Prediction\n",
    "\n",
    "X = \"trump was president in 2017\"\n",
    "\n",
    "sequences = tokenizer1.texts_to_sequences([X])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "if model.predict(sequences, verbose=0)[0][0] >= 0.5:\n",
    "    print(\"This news is True\")\n",
    "else:\n",
    "    print(\"This news is False\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436332da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87e57c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This news is False\n"
     ]
    }
   ],
   "source": [
    "# Sample Prediction\n",
    "\n",
    "X = \"trump is elected for pressident for africa  \"\n",
    "\n",
    "sequences = tokenizer1.texts_to_sequences([X])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "if model.predict(sequences, verbose=0)[0][0] >= 0.5:\n",
    "    print(\"This news is True\")\n",
    "else:\n",
    "    print(\"This news is False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e7f0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "model.save(\"fake_news_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
